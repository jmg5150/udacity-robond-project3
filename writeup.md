## Project 3: Perception Pick & Place
Project submission for the Udacity Robotics Software Engineer Nanodegree

Jonathan Georgino
January 22 2018

---

### This project writeup is based on the sample template provided in the base repo provided by Udacity.

---

[//]: # (Image References)
[confusionmatrix]: ./figures/confusionmatrix.png
[confusionmatrixhsv]: ./figures/confusionmatrixhsv.png
[confmatrix_project_25]: ./figures/confmatrix_project_25.png
[confmatrix_project_45]: ./figures/confmatrix_project_45.png
[extra_snacks]: ./figures/extrasnacks.png
[testworld1]: ./figures/testworld1.png
[testworld2]: ./figures/testworld2.png
[testworld3]: ./figures/testworld3.png

![testworld3]

## [Rubric](https://review.udacity.com/#!/rubrics/1067/view) Points 

---
### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  

This document is intended to fullfil the requirement of the Writeup / Readme.

### Exercise 1, 2 and 3 pipeline implemented

#### 1. Complete Exercise 1 steps. Pipeline for filtering and RANSAC plane fitting implemented.

Please see the code in the `RANSAC.py` file of the `Exercises` directory for my solution to Exercise 1. Note the `LEAF_SIZE = 0.01` was obtained by experimentation.

#### 2. Complete Exercise 2 steps: Pipeline including clustering for segmentation implemented.  

Please see the code in the `segmentation.py` file of the `Exercises` directory for my solution to Exercise 2, which builds off the RANSAC code developed in Exercise 1. Note that all steps for cluster segmentation have been added to the `pcl_callback()` function in the template Python script to arrive at my solution. Experimentation was used to determine the value for `max_distance = 0.03` on line 52, and for the values for `MinClusterSize`, `MaxClusterSize`, and `ClusterTolerance` on lines 74 -76, as shown below.

```python
    ec.set_ClusterTolerance(0.05)
    ec.set_MinClusterSize(200)
    ec.set_MaxClusterSize(1200)
 ```

#### 3. Complete Exercise 3 Steps.  Features extracted and SVM trained.  Object recognition implemented.

Please see the code in the `features.py` file of the `Exercises` directory for my solution to Exercise 3. Note that both `compute_color_histograms()` and `compute_normal_histograms()` functions have been filled out according to instruction / lesson content. The following figures show the raw and normalized confusion matrix which was generated by the `train_svm.py` script as result of the object training under two different test cases. Note that for both test cases shown below, I increased the number of samples for each object in the training dataset from 5 to 15. 

I also did two passes of the `capture_features.py` script. In the first pass, `using_hsv` parameter was set to `false` such that the training dataset would be constructed using the RGB colorspace. The following confusion matrix was obtained:

![confusionmatrix]

In the second pass, I updated the `using_hsv` parameter was to `true` such that the HSV colorpace was used to construct the training dataset. The following confusion matrix was obtained:

![confusionmatrixhsv]

The table below compares the quality of the two training models:

| Color Space | Features In Training Set | Accuracy        | Accuracy Score |
|-------------|--------------------------|-----------------|----------------|
| RGB         | 105                      | 0.70 (+/- 0.20) | 0.70476        |
| HSV         | 105                      | 0.87 (+/- 0.04) | 0.86667        |

As show in the table above, it can be seen that there is a clear advantage to using HSV color space for this type of object recognition activity.

The `object_recognition.py` file in the `Exercises` directory contains the code which then uses this training data to label the objects in the point cloud.

### Pick and Place Setup

#### 1. For all three tabletop setups (`test*.world`), perform object recognition, then read in respective pick list (`pick_list_*.yaml`). Next construct the messages that would comprise a valid `PickPlace` request output them to `.yaml` format.

The first step in developing a solution to this project was to extract features and train an SVM model on the new objects. The list of objects (shown below) was compiled by reviewing the `pick_list_*.yaml` files in the `/pr2_robot/config/` directory.

| object       | pick_list_1 | pick_list_2 | pick_list_3 |
|--------------|-------------|-------------|-------------|
| biscuits     | Yes         | Yes         | Yes         |
| soap         | Yes         | Yes         | Yes         |
| soap2        | Yes         | Yes         | Yes         |
| book         | No          | Yes         | Yes         |
| glue         | No          | Yes         | Yes         |
| sticky_notes | No          | No          | Yes         |
| snacks       | No          | No          | Yes         |
| eraser       | No          | No          | Yes         |

In order to create the training set for these new objects, I created a copy of the `capture_features.py` file and renamed it to `capture_features_project.py`. I updated the list of models on to match the items shown above.

```python
models = [\
   'biscuits',
   'soap',
   'soap2',
   'book',
   'glue',
   'sticky_notes',
   'snacks',
   'eraser'
   ]

```

Another modification that I did was increase the for loop for each object to take 25 samples in hopes that it will build a very strong model. Per the data presented above in this writeup, I left the `using_hsv` parameter set to `True` for best performance. The final modification to this script was to save the file under a new file name:
`pickle.dump(labeled_features, open('training_set_project.sav', 'wb')`

Upon completion of the running the `capture_features_project.py` script, I made a copy of the `train_svm.py` script and rightly called it `train_svm_project.py`. The only modification necessary for this script was to update the filename to `training_set_project.sav`. The following figure shows the resulting confusion matrix.

![confmatrix_project_25]

I was a bit disappointed to see that the accuracy came in just below 90%. Not wanting to get the project off to a poor start, I decided to update the `capture_features_project.py` code to sample each object 45 times and then rerun the script. With the increased sample size, the following figure shows the confusion matrix:

![confmatrix_project_45]

With an accuracy slightly above 92%, that's much better! The table below shows a comparison between the two training runs:

| Samples Per Object | Features In Training Set | Accuracy        | Accuracy Score |
|--------------------|--------------------------|-----------------|----------------|
| 25                 | 200                      | 0.89 (+/- 0.12) | 0.894472       |
| 45                 | 360                      | 0.92 (+/- 0.11) | 0.922222       |

I will proceed ahead with the training dataset built with 45 samples of each object.

The next step was to write a ROS node and subscribe to /pr2/world/points topic. Following the project guide, I created `perception.py` based off the `project_template.py` file provided in the repository and ported over the code from Exercise 3. I added in the filter to remove outliers using k_mean filtering before doing the Voxel Grid Downsampling, per the template.

```python
    # Much like the previous filters, we start by creating a filter object: 
    outlier_filter = cloud.make_statistical_outlier_filter()

    # Set the number of neighboring points to analyze for any given point
    outlier_filter.set_mean_k(50)

    # Set threshold scale factor
    x = 1.0

    # Any point with a mean distance larger than global (mean distance+x*std_dev) will be considered outlier
    outlier_filter.set_std_dev_mul_thresh(x)

    # Finally call the filter function for magic
    cloud_no_stat_outliers = outlier_filter.filter()
 ```

At this point, I was able to run the code and start to observe it's performance on the first test world. No errors or otherwise major issues, however I noticed that the labeling of the items wasn't quite accurate yet - in fact, it was seeing extra objects. This lead me to begin improving the performance by tweaking the tolerances of the clustering. One challenge I noticed was preventing the points in the cloud from the bin on the left of the robot from being detected as an object, such as in the screenshot below where the robot is seeing this as `snacks` but it should not be recognized as an item.

![extra_snacks]

Ultimately, I decided to add another passthrough filter to the code, this time for the 'y' axis. I played with the `axis_min` and `axis_max` settings and experimented until I was able to remove the corner of the bin that was in the field of view. I also learned that it's possible to see the XYZ coordinate points in RViz using the Selection panel. This helped me to verify that I was in fact filtering for the correct axis and that the `axis_min` and `axis_max` were being set appropriately. This then got rid of the false positives on a fourth object.

Further refinement on the clustering and segmentation parameters got me to the point where the recognition was working well in all three worlds. In fact, it was able to correctly identify all the objects in worlds 1 and 3, and just one object identified incorrectly in test world 2. That darn book is always perceived to be biscuits, maybe the pr2_robot is always hungry ;-)

**TestWorld1 - 3/3**
![testworld1]

**TestWorld2 - 4/5**
![testworld2]

**TestWorld3 - 8/8**
![testworld3]

After getting the object recognition to this level, I took a break from the project and returned the next day to begin implementing the generation of the `.yaml` files. During this phase of the project development, I wrote the code in `pr2_mover()` function in the `perception.py` file. Stepping through this function, the first thing I did was initialize the variables necessary to achieve the goal. [Note that I did not attempt to actually have the PR2 robot perform the placements, my goal was just to generate the `.yaml` output files.]

```python
# TODO: Initialize variables
test_scene_num = Int32()
arm_name = String()
obj_name = String()
pick_pose = Pose()
place_pose = Pose()

place_pose_red = Pose()
place_pose_green = Pose()

dict_list = []

yaml_filename = ''
test_scene_num.data = 3

if test_scene_num.data == 1:
    yaml_filename = 'output_1.yaml'

elif test_scene_num.data == 2:
    yaml_filename = 'output_2.yaml'

elif test_scene_num.data == 3:
    yaml_filename = 'output_3.yaml'
else:
    yaml_filename = 'output_unknown.yaml'

```

The datatypes used for the variables above comes from inspecting the associated ros messages. I also used an `if` statement to set the output file name accordingly. The next step was to get the information from the object pick list as well as the dropbox information, both of which are available via the ros parameter server. The following lines of code are used to retreive this data. I also capture and store the pose information for each of the dropboxes for use later in the loops so that I don't need to look up this information for every single object on the pick list.

```python
# TODO: Get/Read parameters
object_list_param = rospy.get_param('/object_list')
dropbox_param = rospy.get_param('/dropbox')

# TODO: Parse parameters into individual variables
place_pose_red.position.x = dropbox_param[0]['position'][0]
place_pose_red.position.y = dropbox_param[0]['position'][1]
place_pose_red.position.z = dropbox_param[0]['position'][2]

place_pose_green.position.x = dropbox_param[1]['position'][0]
place_pose_green.position.y = dropbox_param[1]['position'][1]
place_pose_green.position.z = dropbox_param[1]['position'][2]
```

At this point, I have everything I need to begin iterating through the list of objects. With each iteration of the loop, the desired object name is compared to the list of items discovered in the environment. Upon matching the object names, the centroid of the desired object is computed, formatted appropriately, and then placed into the `pick_pose` datatype. Following this, the `arm_name` is assigned to be `left` or `right` based on the target bin color (`red` or `green`) as specified in the pick list. At this point, the `place_pose` can also be assigned, which was already read in and stored. Now the dictionary is created using the provided `make_yaml_dict()` function by passing the parameters mentioned above, and added to the list that's being kept for each item on the pick list. Finally, after all objects on the pick list have been traversed in the loop, `send_to_yaml()` function is called to create the desired output file for the world.

```python
# TODO: Loop through the pick list
for i in range(0, len(object_list_param)):
    
    object_name = object_list_param[i]['name']
    object_group = object_list_param[i]['group']

    print ("%d : %s in group %s" % (i, object_name, object_group))

    for obj in object_list:

        if obj.label == object_name:
            obj_name.data = object_name

            # TODO: Get the PointCloud for a given object and obtain it's centroid
            obj_points_arr = ros_to_pcl(obj.cloud).to_array()
            obj_centroid_float = np.mean(obj_points_arr, axis=0)[:3]

            pick_pose.position.x = np.asscalar(obj_centroid_float[0])
            pick_pose.position.y = np.asscalar(obj_centroid_float[1])
            pick_pose.position.z = np.asscalar(obj_centroid_float[2])
            
            # TODO: Assign the arm to be used for pick_place
            # TODO: Create 'place_pose' for the object
            if object_group == 'green':
                arm_name.data = 'right'
                place_pose = place_pose_green
            else:
                arm_name.data = 'left'
                place_pose = place_pose_red

            print ("Creating dictionary for %s" % (obj_name.data))

            # TODO: Create a list of dictionaries (made with make_yaml_dict()) for later output to yaml format
            yaml_dict = make_yaml_dict(test_scene_num, arm_name, obj_name, pick_pose, place_pose)
            dict_list.append(yaml_dict)

# TODO: Output your request parameters into output yaml file
send_to_yaml(yaml_filename, dict_list)

```

The output files generated by this script are included in the repository for review. 

A comparison between `pick_list_1.yaml` and `output_1.yaml` shows that all 3 objects were identified and picked in the correct order and placed into the desired bin.

A comparison between `pick_list_2.yaml` and `output_2.yaml` shows that the book was incorrectly identified as biscuits, and as such, the two items perceived to be biscuits were picked first, followed by the remaining list items in the correct order, skipping over the book item which the robot did not perceive to be in the world.

A comparison between `pick_list_3.yaml` and `output_3.yaml` shows that all 8 objects were identified and picked in the correct order and placed into the desired bin.

Overall, I am satisfied with the results of the project, however there are some clear areas that could be improved. I believe that by increasing the number of sample images in the training dataset could enable the robot to detect the book in world2. There are also some few instances in world3 where the sticky_notes are errantly recognized as soap2, which I believe would also be improved with a larger set of training data.

Another point of improvement would be such after an object has been found when looping through the pick list, it break out of the loop and advances to the next item. This would enable a more graceful solution when objects in the world are errantly recognized multiple times. For example, this would mean for my world2 trial that only one box of biscuits would be placed into the bin, instead of it placing two objects into the bin for this single item on the pick list.

Lastly, I could have taken the time to go the extra mile and implement the collision mapping and actual transmission of the ROS messages to the robot to try to actually carry out the pick and place movements in the simulated environment. However running this project in VMWare is quite taxing on my laptop and it's not a very enjoyable development environment as it runs painfully slow. With a better development environment, I would be much more interested in pursuing these challenging tasks.


