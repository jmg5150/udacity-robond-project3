## Project 3: Perception Pick & Place
Project submission for the Udacity Robotics Software Engineer Nanodegree

Jonathan Georgino
January 17 2018

---

### This project writeup is based on the sample template provided in the base repo provided by Udacity.

---

[//]: # (Image References)
[confusionmatrix]: ./figures/confusionmatrix.png
[confusionmatrixhsv]: ./figures/confusionmatrixhsv.png
[figure2]: ./figures/figure2.png 

TODO: Write up summary in this section

## [Rubric](https://review.udacity.com/#!/rubrics/1067/view) Points 

---
### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  

This document is intended to fullfil the requirement of the Writeup / Readme.

### Exercise 1, 2 and 3 pipeline implemented

#### 1. Complete Exercise 1 steps. Pipeline for filtering and RANSAC plane fitting implemented.

Please see the code in the `RANSAC.py` file of the `Exercises` directory for my solution to Exercise 1. Note the LEAF_SIZE = 0.01 was obtained by experimentation.

#### 2. Complete Exercise 2 steps: Pipeline including clustering for segmentation implemented.  

Please see the code in the `segmentation.py` file of the `Exercises` directory for my solution to Exercise 2, which builds off the RANSAC code developed in Exercise 1. Note that all steps for cluster segmentation have been added to the `pcl_callback()` function in the template Python script to arrive at my solution. Experimentation was used to determine the value for `max_distance = 0.03` on line 52, and for the values for `MinClusterSize`, `MaxClusterSize`, and `ClusterTolerance` on lines 74 -76, as shown below.

```python
    ec.set_ClusterTolerance(0.05)
    ec.set_MinClusterSize(200)
    ec.set_MaxClusterSize(1200)
 ```

#### 3. Complete Exercise 3 Steps.  Features extracted and SVM trained.  Object recognition implemented.

Please see the code in the `features.py` file of the `Exercises` directory for my solution to Exercise 3. Note that both `compute_color_histograms()` and `compute_normal_histograms()` functions have been filled out according to instruction / lesson content. The following figures show the raw and normalized confusion matrix which was generated by the `train_svm.py` script as result of the object training under two different test cases. Note that for both test cases shown below, I increased the number of samples for each object in the training dataset from 5 to 15. 

I also did two passes of the `capture_features.py` script. In the first pass, `using_hsv` parameter was set to `false` such that the training dataset would be constructed using the RGB colorspace. The following confusion matrix was obtained:

![confusionmatrix]

In the second pass, I updated the `using_hsv` parameter was to `true` such that the HSV colorpace was used to construct the training dataset. The following confusion matrix was obtained:

![confusionmatrixhsv]

The table below compares the quality of the two training models:

| Color Space | Features In Training Set | Accuracy        | Accuracy Score |
|-------------|--------------------------|-----------------|----------------|
| RGB         | 105                      | 0.70 (+/- 0.20) | 0.70476        |
| HSV         | 105                      | 0.87 (+/- 0.04) | 0.86667        |

As show in the table above, it can be seen that there is a clear advantage to using HSV color space for this type of object recognition activity.


### Pick and Place Setup

#### 1. For all three tabletop setups (`test*.world`), perform object recognition, then read in respective pick list (`pick_list_*.yaml`). Next construct the messages that would comprise a valid `PickPlace` request output them to `.yaml` format.

You can add this functionality to your already existing ros node or create a new node that communicates with your perception pipeline to perform sequential object recognition. Save your PickPlace requests into output_1.yaml, output_2.yaml, and output_3.yaml for each scene respectively. Add screenshots in your writeup of output showing label markers in RViz to demonstrate your object recognition success rate in each of the three scenarios. Note: for a passing submission, your pipeline must correctly identify 100% of objects in test1.world, 80% (4/5) in test2.world and 75% (6/8) in test3.world.

Spend some time at the end to discuss your code, what techniques you used, what worked and why, where the implementation might fail and how you might improve it if you were going to pursue this project further.  



